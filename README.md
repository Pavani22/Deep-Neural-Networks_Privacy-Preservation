# Privacy-Preserving-Classiffication-in-Deep-Neural-Networks

This project focuses on recreating the neural network and the encrypted accuracy results of paper [1].  We initially recreated the deep neural network using Keras getting approximately the same results as the paper with 98.3% accuracy.  We moved on from this to the caffe framework and integrated the HeLib library to create a high performance privacy preserving neural network implementation in C++.  Once we replicated the results in this environment without privacy preservation we enabled HeLib and updated the configuration files to classify using the new privacy preserving layers added to the caffe framework.  We also updated the configuration files to replicate the learning rate, momentum, learning policy, and other values to the values specified in the paper[1].  Since training is done without privacy preservation we had to setup the configuration file to use the privacy preserving layers for classification and the old methods for training.  Once the privacy preserving layers were used we also had to ensure the model parameters from training were encrypted when we used privacy preservation, since they are used as part of the basic mathematical operations used within each layer.  Our plan was to have the first layer encrypt all data using a public key, and have every subsequent layer just work on encrypted data using the HeLib addition and multiplication operations.  HeLib proved to be very complicated with several drawbacks causing us to have each layer encrypt, do their homomorphic operations on the data, and then decrypt for the next layer.  We only completed the ReLU layer using the privacy preservation mechanisms, but still enough to show its efficacy for this project.
##### HeLib Library
The original paper uses HeLib, so we decided to use an updated version of HeLib with performance improvements to replicate the results.  Default values were used for most key input parameters, but we needed a much larger prime value for the finite field HeLib utilizes in order to ensure our multiplied values did not wrap.  We selected a prime value of 160,000,038,029, and an encrypted security value of 128.  This security value ensures the encrypted data has 128-bit equivalent protection to symmetric ciphers.  With these values we get 2112 available slots in the encrypted context vectors allow for efficient ciphertext packing.  For each encrypted operation we can do the same operation on 2112 unique inputs drastically increasing performance for this implementation.  We tried many different prime numbers and security values but these gave us less benefits in the SIMD performance of this implementation.  We used defaults for other values including the columns for Key Switching Matrices(3), Hamming Weight(64), and Context Levels(16).  All of these values are then used to generate the Homomorphic Encryption context along with the Public/Private Key pair.  We used these three values in our encrypted layer implementations to encode plaintext, encrypt plaintext using the public key, operate on ciphertext, decrypt ciphertext using the private key, and decode the ciphertext.  This HeLib framework works along with GMP and NTL mathematical libraries to provide higher performance computation.  The library uses the BGV encryption scheme which is defined over polynomial rings using a m’th cyclotomic polynomial.  The library provides key generation, encryption, decryption, decoding, encoding, homomorphic evaluation for addition, multiplication, and rotation, and ciphertext operations for key-switching.
##### Caffe Framework
Because the caffe framework was configured to allow easy addition and selection of new layers we implemented new accuracy, batch normalization, convolutional neural network, data, fully connected, average pooling, and ReLU layers.  These new layers would do their normal feed forward operations over encrypted data using the HeLib library methods only for testing classification to simulate cloud classification.  We used the standard layers for training the CNN model, and configured two different CNN models for the configuration files.  The input datasets were converted into the lmdb data format and scaled based on image pixel input range.  For instance the mnist dataset uses 256 bit grayscale values so the input scale value was 0.0039 to scale inputs between 0 and 1.  The training inputs were batch processed 100 images at a time where back propagation occurred after each batch.  One iteration of training consisted of going over all images once, and after every 500 iterations one test classification was ran.  The test cycle ran calculated classification accuracy of the test dataset over a batch size of 64 images at a time, and provided accuracy over all image inputs.  We replicated the network layers, learning parameters, and other inputs exactly from the paper[1].

The new ReLU layer was updated to take in bottom_layer inputs, encrypt them using the HeLib public key, and use the HeLib multiplication/addition overloaded operators to calculate the polynomial approximation to the standard ReLU from the paper using the two degree polynomial (0.1992 + 0.5002X + 0.1997X2).  Each ReLU implementation required us to change out this approximation in the standard ReLU layer and change how we did the homomorphic encryption operations in the encrypted layer.    Floating point inputs were difficult to represent in HeLib which caused us to shift the floating point inputs out of the mantissa to represent them as a signed long value.  This quantization method was effective since there didn’t seem to be a understandable method in HeLib to convert floats.  We shifted all inputs by one to three digits and represent them as signed longs shifting them back accordingly after decryption.  Unfortunately we have to take into account the multiplication depth of the approximation.  In this case the depth is three so we have to divide the results by 10003  to get the final accurate result.  We also needed to represent every value in the approximation by this maximum multiplication depth.  So the first constant 0.1992 is now represented as 0.1992 * 1000 * 1000 * 1000.  Since we are always only as accurate as three decimal points we saw up to 2% variance in the decrypted output compared to expected output.  This had nothing to do with HeLib, but our method of using HeLib.  Depending on the input values we couldn’t use this degree of accuracy, and updated the encrypted ReLU layer to dynamically determine the degree we could use without allowing values to wrap during encrypted mathematical operations.  Parallelization was used to double the efficiency of HeLib by using OpenMP to make HeLib calculation multithreaded.  This could easily be parallelized more up to potentially 24 threads/cores.

We chose three other ReLU approximations to run against the datasets including leaky ReLU implementations.  After collecting results for both encrypted and unencrypted accuracies we collected data using the same methods using the CIFAR-10 dataset.  We had to make some minor updates to the configuration files to get training to be somewhat accurate.  Other changes would better replicate the results from the paper, and provide a better understanding of our contributions compared to the original paper, but time did not allow this.
